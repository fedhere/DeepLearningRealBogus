\label{stateofart}
\subsection{Difference Imaging}
\label{sec:DI}
Difference images produced subtracting a template generated coadding multiple images \citep[\eg][]{Kessler_2015} from a sky image are currently the basis for most astrophysical transient search algorithms. The difference image allows brightness changes to be detected even if embedded in Galaxy light, for example, in the case of extra-galactic explosive transients. Great efforts have been made to improve the quality and effectiveness of the difference images. Although the name may suggest the process simply entails subtracting images from each other, the procedure is in fact riddled with complications because of the following reasons. First, the images used to build the template and the search images are taken principally within different atmospheric conditions \citep{Zackay_2017} generating variations in the quality of the images. The construction of a proper template is also a delicate task; typically templates are built by stacking tens of images taken under favourable sky conditions at different times. This improves the image quality but also mitigates issues related to variability in the astrophysical objects captured in the image \citep{Hambleton_2020}: one wants to capture each variable source at its representative brightness. Typically then, the template image is of higher quality than the search image, and it is degraded to match the search image PSF and scaled to match its brightness. Yet, the scaling and PSF may vary locally in the image plane, especially for images from large field of view synoptic surveys such as the 2.2 sq. degrees DES or $\sim10$ sq. degrees Rubin images. Finally it is important that the images are perfectly aligned, both in the creation of the template and in the subtraction process to create the difference image. This implies accounting for rotation as well as potentially different warping effects on the images and template. Once the PSF match and the alignment is done, it is possible to subtract the degraded template from the search images to obtain the difference image. To degrade the image quality of the template to match the search image, a convolution kernel that must be applied to the template needs to be determined \citep{Alard_1998},
\begin{eqnarray}
    \text{\temp}(x,y) \otimes \text{Kernel}(u,v) = \text{\search}(x,y),
\end{eqnarray}

\noindent 
where \temp\ is the high-quality image, the template image, \search is the one night image or search image and $\otimes$ is the convolutional operation. The arguments $x$ and $y$ represent the coordinates of matrix that compose the images; $u$ and $v$ the coordinates of the kernel matrix. 

To solve the computationally expensive problem of matching PSFs, the kernel can be decomposed in terms of simple functions, for instance Gaussian functions, and the method of least squares can be used to determine the best values for the kernel. The fitted solution of one search image can be determined in a short computational time. However, the computational cost scales with the image size and resolution. Surveys and telescopes constructed with the goal of discovering new transients are generally designed to collect tremendous amount of data to maximize event rate (detection of astrophysical transients).  For the DES, the computational cost per 2.2 sq. degree image $\sim 15.5$ CPU hours (with roughly 2/3 of that time spent on PSF matching).  The upcoming Rubin LSST will collect more than 500 images every night each with 3.2 Gigapixels. This process thus is bound to turn out to be very expensive.
In this paper, we trained our models on postage stamps where transients were detected or simulated (see \autoref{sec:data}), thus a direct comparison of the computational cost is not trivial. A detailed discussion of the computational cost of our models is included in \autoref{sec:computationcost} and the CPU Node hours required to train and generate predictions from our models are reported in \autoref{tab:acc results}. We note here that, with a Deep Neural Network approach to this problem, the computational cost is high in training, but the predictions are essentially instantaneous.

A bad subtraction can occur either because of poor PSF matching, poor alignment, or poor correction of image warping. In all of these cases, the subtraction would lead to artifacts or ``bogus'' alerts, like the one shown in the difference image in  \autoref{fig:examples_no_normalization}A and B. In particular: in \autoref{fig:examples_no_normalization}A, the difference image shows a so-called ``dipole'', where one side of a suspected transient is dark and the other side bright: this typically arises in case of mis-alignments, but it might also be caused by moving objects in the field, or differential chromatic refraction \citep{carrascodavis2021alert}. Conversely, \autoref{fig:examples_no_normalization}B shows a ``bogus'' alert caused by an image artifact: a column of bad pixels in the search image. At this location there is no astrophysical object in the image thumbnail: no host galaxy or star that could give rise to variations.

Panels \autoref{fig:examples_no_normalization}C and \autoref{fig:examples_no_normalization}D show genuine transients in our DES training data: in both of these two examples, there is a clear transient in the \diff\  images (high pixel values at the center of the image).

\subsection{Autoscan and other feature-based Real Bogus models}
\label{sec:autoscan}

We developed our models on data collected in the first year of DES. Thus, a direct precursor of our work is \cite{Goldstein_2015}, in which the authors created an automated RB based on a Random Forest (RF) supervised learning model \citep{ho1995random} to detect transients, and particularly supernovae, in the DES data, hereafter refered to as \texttt{autoscan}. For these kinds of models, the process of selecting and engineering features is pivotal. %\citep{2011AAS...21720507S} (\masao{I suggest removing this reference}).
% \footnote{The data can be found at \url{https://portal.nersc.gov/project/dessn/autoscan/\#}} 
\texttt{Autoscan} is based on 38 features derived from the \diff, \search\ and \temp\ images. The selection and computation of these features was done attempting to represent quantitatively what humans would leverage in visual inspections. For instance, \texttt{r\_aper\_psf}  distinguishes a bad subtraction of \search\ and \temp\ that would lead to a \diff\ qualitative similar to  \autoref{fig:examples_no_normalization}A; the feature \texttt{diffsum} measures the significance of the detection by summing the pixel values in the center of the \diff\ image; the feature \texttt{colmeds}, indicating the CCD used for the detection, is designed to identify artifacts specific of a CCD, like bad rows/columns of pixels.


In other RB models, like in \cite{S_nchez_2019},
the feature selection is performed purely statistically: features were initially selected based on variance thresholds. In the same work, different techniques are explored to reduce the number of features, and thus the complexity of the classification problem. For example, a RF model was trained using all features. Then, feature values were randomly modified and the performance on the ``new'' data set was analyzed. Decreased accuracy associated to a change of a feature measured the importance of the feature that was modified. In this way the authors reduced the dimensionality of the problem by removing possible redundant or irrelevant information. Examples of models based on features closer to the data include \cite{Mong_2020}, where the features are simply the flux values of the pixels around the center of the image.




\subsection{AI approach}\label{sec:AI}


CNNs have demonstrated enormous potential in image analysis including object detection, recognition, and classification across domains \citep{5206848}. Examples of astrophysics applications of CNNs include \citet{Dieleman_2015} for galaxy morphology prediction, \citet{Kim_2016} for star-galaxy classification, \citet{PhysRevLett.120.141103} for signal/background separation for Gravitational Waves (GW) searches, where the GW time series are purposefully encoded as images to be analyzed by a CNN, and many more.


CNNs are particularly well-suited to learning discriminating features from image input data. CNNs can work on high dimensional spaces (here the dimensionality of the input is as large as the number of pixels in the image) due to the generalizability of the convolution operation to $n$ dimensions while preserving relative position information. Vectors of raw pixel values can theoretically be used to train traditionally feature-based models, such as RFs, but pixel-to-pixel position data in higher dimensions is unequivocally lost. Previous studies already compared feature-based supervised models, like RF, and supervised CNNs for RB, demonstrating that CNN generally leads to increased accuracy.  In \cite{Gieseke_2017} an accuracy of $\sim0.984$ is achieved with a RF model in the RB task, and it increases to $\sim0.990$ when applying a CNNs to the same data. In \cite{Cabrera_2016}  accuracy of $\sim 0.9889$ achieved with a RF is increased to to $\sim0.9932$ with a CNN. In \cite{Cabrera_Vives_2017}, an RF gave 0.9896 and a CNN 0.9945. In \citet{Liu_2019} the accuracy improves from $\sim0.9623$ with a RF to $\sim0.9948$ with a CNN.

Beyond RB, a model for image-based transient classification through CNNs has been prototyped by the Automatic Learning for the Rapid Classification of Events team \citep[ALeRCE][]{carrascodavis2021alert}. The model classifies between AGNs (Active Galactic Nuclei), SNe (SuperNovae), variable stars, asteroids, and artifacts in The Zwicky Transient Factory \citep[ZTF]{Bellm_2018} survey data with a reported accuracy exceeding 95\% for all types, except SNe (87\%). This CNN model was trained using a combination of the \search, \temp, and \diff.%, where rotational invariance was considered by training the model with rotated version of the images and features that characterize each astronomical object type were passed through a layer and concatenated with the output layer of the images. %They results have shown that AGN can be miss-classify for VS and vice versa; SN for asteroids or bogus; asteriods for SN or VS, and bogus for all of them except AGN.

The problem we propose to address here is exclusively the RB classification, but we explore the potential of leveraging AI to bypass the DIA step. 
Existing CNN RB models differ not only in their architecture (for example, a single or multiple sequences of convolutional, pooling, dropout, and dense layers) but also in the choice of input. For instance, \cite{Gieseke_2017} used the \temp, \search, and \diff\ images in combination for training their CNN; \cite{Cabrera_2016} augmented this image set with an image generated as the \diff\ divided by an estimate of the local noise;  \cite{Cabrera_Vives_2017} trained an ensemble of CNNs on different rotations of this four-fold image set. Conversely, \citet{Liu_2019} used the \diff\ as the sole input. Although, all these attempts have shown good results with accuracy higher than $90\%$, these models all rely on DIA to construct the \diff. Taking into account that \diff\ are built from the \temp\ and \search, and in principle, should carry no additional information content than the search-template pair alone, a logical step to follow would be to only consider the two latter images.  


\tatiana{A first attempt in this direction is presented in \cite{Sedaghat_2018}, where the authors develop a Convolutional Autoencoder (encoder-decoder) named TransiNet. The model is developed and tested on both real and synthetic data. Synthetic data was created by using background images from the Galaxy Zoo data set in Kaggle \question{CITE}, then simulated transients were implanted in the search images. Template and search images from the Supernova Hunt project (Catalina Real-time Transient Survey ---CTRS--- \question{CITE}) were also used. Data were fed to the autoencoder to generate a difference image that contains only the transient (the CNN do not generate background noise). 
The CNN model was trained and tested only on synthetic data and separately trained on a combination of synthetic and real data and tested on the real data. The former model achieves scores (precision and recall) of 100\%; the latter model a precision of 93.4\% and recall of 75.5\%, and establish a precedent for the possibility of avoiding the construction of the DIA-\diff\ to reliably detect optical transients.

Another notable work where difference images are not used is \cite{Carrasco_Davis_2019}. The authors implement Recurrent convolutional
Neural Network (RCNN) to train a sequence of images (instead of the classical template and search images) to classify 7 types of variable objects. The model was trained using syntethic data and tested using data from the High cadence Transient Survey (HiTS \red{CITE}). The average performance recall of the model is 94\%.}

\cite{wardega2020detecting} trained a model that could distinguish between optical transient and artifacts using a search image from Dr. Cristina V. Torres Memorial Astronomical Observatory (CTMO, a facility of the University of Texas Rio Grande Valley\footnote{\url{https://www.utrgv.edu/physics/outreach/observatory/index.htm}}) and a template image from the Sloan Digital Sky Survey \citep[SDSS]{Gunn_2006}. They trained two Artificial Neural Network models, a CNN and a Dense Layer Network on simulated data and tested the models using data from CTMO and SDSS. The data used for training and testing had specific characteristics: transients were a combination of a source in the CTMO images (or \search\ image) and background in the SDSS image (or \temp\ image); artifacts were a combination of a source in both the CTMO and SDSS images. Within this dataset, both models yield high accuracy ($>95\%$).  However, studies based on more diverse and realistic data, \eg, sources near galaxies, or embedded in clusters, are needed to demonstrate the feasibility of this approach. The data used in this paper fulfills this condition and is described in \autoref{sec:data}.
